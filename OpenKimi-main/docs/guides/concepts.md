# 核心概念

OpenKimi 通过一系列创新的技术突破了传统 LLM 的上下文长度限制。本文将介绍 OpenKimi 的核心概念和工作原理。

## 无限上下文处理的基本原理

传统 LLM 的上下文长度有硬性限制（如 4K、8K 或 16K tokens），这限制了它们处理长文本的能力。OpenKimi 通过以下核心机制突破这一限制：

1. **文本处理与信息熵评估**
2. **递归 RAG（检索增强生成）**
3. **框架生成 & MPR（多路径推理）**

### 文本处理和信息熵计算

当输入长文本时，OpenKimi 会：

1. **分块**：将长文本切分为可管理的批次（默认为 512 tokens 大小）
2. **信息熵评估**：计算每个文本块的信息熵（信息密度）
3. **分类**：将文本块分为高信息熵（有用信息密集）和低信息熵（信息稀疏）两类

```
高信息熵文本 → 保留在即时上下文中
低信息熵文本 → 存入 RAG 系统，生成摘要并向量化
```

### RAG 管理（向量检索）

对于低信息熵的文本：

1. **摘要生成**：使用 LLM 为每个低信息熵文本块生成简短摘要
2. **向量化**：使用 Sentence Transformers 模型将摘要转换为向量表示
3. **存储**：将原文和摘要向量存储在 RAG 系统中
4. **检索**：当需要上下文时，通过计算查询与摘要向量的相似度，检索最相关的文本块

这样，只有当真正需要时，低信息熵的内容才会被检索出来，大幅降低内存占用。

### 递归 RAG 压缩

当构造的 Prompt 超过模型上下文长度限制时，OpenKimi 会触发递归 RAG 压缩：

1. **分块与熵计算**：将过长文本分块并计算信息熵
2. **临时 RAG 存储**：将低信息熵部分存入临时 RAG 系统，生成摘要
3. **压缩文本构建**：保留高信息熵部分和低信息熵的摘要，形成压缩后的文本
4. **递归处理**：如果压缩后的文本仍然过长，递归重复上述步骤

通过这种方式，无论输入多长，OpenKimi 都能构造出符合 LLM 上下文限制的高质量 Prompt。

### 框架生成 & MPR

OpenKimi 使用两阶段生成方法来提高复杂问题的解决质量：

1. **框架生成**：将复杂问题分解为逻辑步骤
2. **MPR（多路径推理）**：
   - 为同一问题生成多个候选解决方案（基于不同上下文侧重）
   - 通过最终的 LLM 调用将候选方案综合为一个全面的答案

MPR 提高了回答的全面性和鲁棒性，特别是对于复杂问题。

## 模块架构

OpenKimi 的架构由以下核心模块组成：

### KimiEngine

主引擎，整合所有模块，提供对话接口：

- **ingest(text)**：摄入和处理长文本
- **chat(query)**：处理用户查询并生成回复
- **reset()**：重置会话历史和 RAG 存储

### TextProcessor

处理输入文本，计算信息熵，并进行分类：

- **split_into_batches(text)**：将文本分块
- **classify_by_entropy(batches, threshold)**：基于信息熵分类文本块

### RAGManager

管理向量存储和检索系统：

- **batch_store(texts)**：批量存储文本并生成摘要向量
- **retrieve(query, top_k)**：检索与查询最相关的文本

### FrameworkGenerator

生成解决方案框架和最终答案：

- **generate_framework(query, context)**：生成问题解决框架
- **generate_solution_mpr(query, framework, context, rag_context, num_candidates)**：
  使用 MPR 生成最终解决方案

### LLMInterface

与底层 LLM 交互的统一接口，支持多种后端：

- **dummy**：用于测试
- **local**：使用 Hugging Face Transformers 加载本地模型
- **api**：兼容 OpenAI Chat Completion API

## 配置选项

OpenKimi 提供了灵活的配置选项，可以通过 JSON 配置文件或初始化参数调整：

- **LLM 配置**：模型类型、路径、设备、上下文长度等
- **处理器配置**：批次大小、熵阈值
- **RAG 配置**：嵌入模型、top_k 检索数量
- **MPR 配置**：候选方案数量

详细配置参数请参考 [API 参考](../api/index.md)。

## 工作流程示例

以下是 OpenKimi 处理长文本和查询的典型工作流程：

1. **初始化**：创建 KimiEngine 实例，加载配置
2. **文本摄入**：
   - 分块、计算信息熵、分类
   - 高信息熵部分保留在上下文，低信息熵部分存入 RAG
3. **用户查询**：
   - 从 RAG 检索相关内容
   - 结合近期对话历史构建上下文
   - 生成解决方案框架
   - 使用 MPR 生成最终答案
4. **递归 RAG 压缩**：在需要时自动触发，确保所有 Prompt 符合模型上下文限制 